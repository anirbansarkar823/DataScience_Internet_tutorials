# -*- coding: utf-8 -*-
"""biasInMachineLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Ojvhr_exs09UZmGzBKX3PVvbHXhwlZG

# Mitigating Bias in AI with AIF360
This jupyter notebook is accompanied by a Medium article "bryantruong" wrote, published [here](https://bryantruong3139.medium.com/mitigating-bias-in-ai-with-aif360-b4305d1f88a9):

### Loading and preparing the dataset

### Data Import
**Importing data from google drive**
"""

# importing from google drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
path = "/content/drive/MyDrive/Colab Notebooks/credit_risk.csv"
df = pd.read_csv(path)
# Dataset is now stored in a Pandas Dataframe
df.head()

!pip install aif360

# ModuleNotFoundError: No module named 'fairlearn'; required for aif360
!pip install fairlearn

# First, read-in the data and check for null values
import numpy as np
import pandas as pd
import aif360
from aif360.algorithms.preprocessing import DisparateImpactRemover
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
pd.options.mode.chained_assignment = None  # default='warn', silencing Setting With Copy warning

# See the different columns and check for null entries
df.info()

# Let’s take a look at the dataset’s features:
# 1. Loan ID
# 2. Gender (Male or Female)
# 3. Married (Yes or No)
# 4. Dependents (0, 1, 2, or 3+)
# 5. Education (indicating whether or not the primary applicant has graduated from high school)
# 6. Self Employed (Yes or No)
# 7. Applicant Income
# 8. Co-Applicant Income
# 9. Loan Amount
# 10. Loan Applicant Term
# 11. Credit History (0 or 1, with 0 indicating good credit history)
# 12. Property Area (Rural, Semiurban, or Urban)
# 13. Loan Status (Y or N)

# Remove rows with any (even if a single) null values
df = df.dropna(how='any', axis = 0) 
df.info()

"""The author then wants to check to see the breakdown of values for the outcome variable, `Loan_Status`."""

target_counts = df['Loan_Status'].value_counts()
target_counts

#the data seems to be biased as more data points for Y

# Drop unnecessary column
df = df.drop(['Loan_ID'], axis = 1)

"""### Encode categorical variables: a new way to do it"""

# Encode Male as 1, Female as 0
df.loc[df.Gender == 'Male', 'Gender'] = 1
df.loc[df.Gender == 'Female', 'Gender'] = 0

# Encode Y Loan_Status as 1, N Loan_Status as 0
df.loc[df.Loan_Status == 'Y', 'Loan_Status'] = 1
df.loc[df.Loan_Status == 'N', 'Loan_Status'] = 0
df

y = df['Loan_Status']
y

[col for col in df.columns if df[col].dtype in ['object']]

# Replace the categorical values with the numeric equivalents that we have above
categoricalFeatures = ['Property_Area', 'Married', 'Dependents', 'Education', 'Self_Employed']

# Iterate through the list of categorical features and one hot encode them.
for feature in categoricalFeatures:
    onehot = pd.get_dummies(df[feature], prefix=feature)
    df = df.drop(feature, axis=1)
    df = df.join(onehot)
df

"""### Separate dataset by x and y"""

from sklearn.model_selection import train_test_split
encoded_df = df.copy()
x = df.drop(['Loan_Status'], axis = 1)

"""### Create Test and Train splits"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data_std = scaler.fit_transform(x)

# converting to dataframe
data_std = pd.DataFrame(data_std, index=x.index, columns=x.columns)

# We will follow an 80-20 split pattern for our training and test data, respectively
x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2, random_state = 0)

x.head()

data_std.head()

"""### Calculating actual disparate impact on testing values from original dataset
Disparate Impact is defined as the ratio of favorable outcomes for the unpriviliged group divided by the ratio of favorable outcomes for the priviliged group.
The acceptable threshold is between .8 and 1.25, with .8 favoring the priviliged group, and 1.25 favoring the unpriviliged group.
"""

y_test.head()

type(y_test)

actual_test = x_test.copy()
actual_test['Loan_Status_Actual'] = y_test
print(actual_test.shape)
print(x_test.shape)

actual_test.info()

actual_test['Gender'].unique()

# Priviliged group: Males (1)
# Unpriviliged group: Females (0)
# privilege
male_df = actual_test[actual_test['Gender'].astype('int') == 1]
num_of_priviliged = male_df.shape[0]
print(num_of_priviliged)

# unprivilege
female_df = actual_test[actual_test['Gender'].astype('int') == 0]
num_of_unpriviliged = female_df.shape[0]
print(num_of_unpriviliged)

unpriviliged_outcomes = female_df[female_df['Loan_Status_Actual'] == 1].shape[0]
unpriviliged_ratio = unpriviliged_outcomes/num_of_unpriviliged
unpriviliged_ratio

priviliged_outcomes = male_df[male_df['Loan_Status_Actual'] == 1].shape[0]
priviliged_ratio = priviliged_outcomes/num_of_priviliged
priviliged_ratio

# Calculating disparate impact
disparate_impact = unpriviliged_ratio / priviliged_ratio
print("Disparate Impact, Sex vs. Predicted Loan Status: " + str(disparate_impact))

"""### Training a model on the original dataset"""

from sklearn.linear_model import LogisticRegression
# Liblinear is a solver that is very fast for small datasets, like ours
model = LogisticRegression(solver='liblinear', class_weight='balanced')

# y is of type object, so sklearn cannot recognize its type. Add the line y=y.astype('int')
y_train=y_train.astype('int')
model.fit(x_train, y_train)

"""### Evaluating performance"""

# Let's see how well it predicted with a couple values 
y_pred = pd.Series(model.predict(x_test))
y_test = y_test.reset_index(drop=True)
z = pd.concat([y_test, y_pred], axis=1)
z.columns = ['True', 'Prediction']
z.head()
# Predicts 4/5 correctly in this sample

print(type(y_pred))
print(type(y_test))

print(y_pred.dtype)
print(y_test.dtype)

print(y_test.unique())
print(y_pred.unique())
y_pred.head()

print(x_test.shape)

import matplotlib.pyplot as plt
from sklearn import metrics
print("Accuracy:", metrics.accuracy_score(y_test.astype('int'), y_pred))
print("Precision:", metrics.precision_score(y_test.astype('int'), y_pred))
print("Recall:", metrics.recall_score(y_test.astype('int'), y_pred))

"""### Calculating disparate impact on predicted values by model trained on original dataset"""

# We now need to add this array into x_test as a column for when we calculate the fairness metrics.
y_pred = model.predict(x_test)
x_test['Loan_Status_Predicted'] = y_pred
original_output = x_test
#x_test.drop(['Loan_Status_Predicted'], axis=1, inplace=True)
original_output

# Priviliged group: Males (1)
# Unpriviliged group: Females (0)
male_df = original_output[original_output['Gender'] == 1]
num_of_priviliged = male_df.shape[0]
female_df = original_output[original_output['Gender'] == 0]
num_of_unpriviliged = female_df.shape[0]

unpriviliged_outcomes = female_df[female_df['Loan_Status_Predicted'] == 1].shape[0]
unpriviliged_ratio = unpriviliged_outcomes/num_of_unpriviliged
unpriviliged_ratio

priviliged_outcomes = male_df[male_df['Loan_Status_Predicted'] == 1].shape[0]
priviliged_ratio = priviliged_outcomes/num_of_priviliged
priviliged_ratio

# Calculating disparate impact
disparate_impact = unpriviliged_ratio / priviliged_ratio
print("Disparate Impact, Sex vs. Predicted Loan Status: " + str(disparate_impact))

"""### Applying the Disparate Impact Remover to the dataset"""

# We are going to be using the dataset with categorical features encoded, encoded_df
encoded_df

encoded_df.info()

encoded_df.Loan_Status.unique()

!pip install BlackBoxAuditing

import aif360
from aif360.algorithms.preprocessing import DisparateImpactRemover
# binaryLabelDataset = aif360.datasets.BinaryLabelDataset(
#     df=yourDataFrameHere,
#     label_names=['yourOutcomeLabelHere'],
#     protected_attribute_names=['yourProtectedClassHere'])
# Must be a binaryLabelDataset
binaryLabelDataset = aif360.datasets.BinaryLabelDataset(
    favorable_label=1,
    unfavorable_label=0,
    df=encoded_df,
    label_names=['Loan_Status'],
    protected_attribute_names=['Gender'])
di = DisparateImpactRemover(repair_level = 1.0) # require !pip install BlackBoxAuditing
dataset_transf_train = di.fit_transform(binaryLabelDataset)
transformed = dataset_transf_train.convert_to_dataframe()[0] #converted to dataframe
transformed

"""### Train a model using the dataset that underwent the pre-processing"""

x_trans = transformed.drop(['Loan_Status'], axis = 1)
y = transformed['Loan_Status']

# Liblinear is a solver that is effective for relatively smaller datasets.
model = LogisticRegression(solver='liblinear', class_weight='balanced')
scaler = StandardScaler()
data_std = scaler.fit_transform(x_trans)

# Splitting into test and training
# We will follow an 80-20 split pattern for our training and test data
x_trans_train,x_trans_test,y_trans_train,y_trans_test = train_test_split(x_trans, y, test_size=0.2, random_state = 0)

model.fit(x_trans_train, y_trans_train)

"""### Evaluating performance"""

# See how well it predicted with a couple values
y_trans_pred = pd.Series(model.predict(x_trans_test))
y_trans_test = y_trans_test.reset_index(drop=True)
z = pd.concat([y_trans_test, y_trans_pred], axis=1)
z.columns = ['True', 'Prediction']
z.head()
# Again, it predicts 4/5 correctly in this sample

print(y_test.dtype)
print(y_trans_pred.dtype)

print("Accuracy:", metrics.accuracy_score(y_test.astype('float'), y_trans_pred))
print("Precision:", metrics.precision_score(y_test.astype('float'), y_trans_pred))
print("Recall:", metrics.recall_score(y_test.astype('float'), y_trans_pred))

"""### Calculating disparate impact on predicted values by model trained on transformed dataset"""

# We now need to add this array into x_test as a column for when we calculate the fairness metrics.
y_trans_pred = model.predict(x_trans_test)
x_trans_test['Loan_Status_Predicted'] = y_trans_pred
transformed_output = x_trans_test

#x_trans_test.drop(['Loan_Status_Predicted'], axis=1, inplace=True)
transformed_output

"""Disparate Impact is defined as the ratio of favorable outcomes for the unpriviliged group divided by the ratio of favorable outcomes for the priviliged group. The acceptable threshold is between .8 and 1.25, with .8 favoring the priviliged group, and 1.25 favoring the unpriviliged group."""

# Priviliged group: Males (1)
# Unpriviliged group: Females (0)
male_df = transformed_output[transformed_output['Gender'] == 1]
num_of_priviliged = male_df.shape[0]

female_df = transformed_output[transformed_output['Gender'] == 0]
num_of_unpriviliged = female_df.shape[0]

# calculation of unprivileged ratio
unpriviliged_outcomes = female_df[female_df['Loan_Status_Predicted'] == 1].shape[0]
unpriviliged_ratio = unpriviliged_outcomes/num_of_unpriviliged
unpriviliged_ratio

# calculation of privileged ration

priviliged_outcomes = male_df[male_df['Loan_Status_Predicted'] == 1].shape[0]
priviliged_ratio = priviliged_outcomes/num_of_priviliged
priviliged_ratio

# Calculating disparate impact
disparate_impact = unpriviliged_ratio / priviliged_ratio
print("Disparate Impact, Sex vs. Predicted Loan Status: " + str(disparate_impact))

# After fixing the bias, we were successfully able to reduce the Disparate Impact, still not in acceptable range