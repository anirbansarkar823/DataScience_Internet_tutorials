{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vlehI0Tk0U3"
   },
   "source": [
    "# Word Embedding In Natural language Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A81X4ukTk0U-"
   },
   "outputs": [],
   "source": [
    "# A word embedding is a learned representation for text where words that have the same meaning have a similar representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_YgK900jk0VA"
   },
   "outputs": [],
   "source": [
    "# Word Embedding - \n",
    "# 1. To provide a meaningful vector representation of the word\n",
    "# 2. This is an improvement over frequency based approaches\n",
    "\n",
    "\n",
    "# Working principle behind word embedding:\n",
    "#  - while Training, meaning of the word is learned by analyzing its neighbors\n",
    "\n",
    "\n",
    "# Usages:\n",
    "# 1. we can use pre-trained models( Word2Vec, glove)\n",
    "# 2. We can also train our own embedding using word2vec library.\n",
    "# 3. training of our own embedding is also possible using Keras embedding method\n",
    "\n",
    "\n",
    "# skip-gram vs cbow (continuous bag of words): While a bag-of-words model predicts a word \n",
    "#                   given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec: https://wiki.pathmind.com/word2vec\n",
    "\n",
    "# word2vec trains words against other words that neighbor them in the input corpus.\n",
    "\n",
    "# It does so in one of two ways, \n",
    "# either using context to predict a target word (a method known as continuous bag of words, or CBOW), \n",
    "# or using a word to predict a target context, which is called skip-gram. it produces more accurate results on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xo9XBob4k0VD"
   },
   "source": [
    "# Option 2 - Training own word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tgSuTvV3k0VE"
   },
   "outputs": [],
   "source": [
    "# define tokenized senences as training data\n",
    "tokenized_sentences = [['Hello','This','is','python','training','by','Aman'],\n",
    "             ['Hello','This','is','Java','training','by','Aman'],\n",
    "             ['Hello','This','is','Data Science','training','by','Unfold','Data','Science'],\n",
    "             ['Hello','This','is','programming','training','']]\n",
    "\n",
    "# '' - this will also be considered as word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vcnJ4M0vk0VF"
   },
   "outputs": [],
   "source": [
    "# training word2vec model : 2. We can also train our own embedding using word2vec library.\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "mymodel = Word2Vec(tokenized_sentences, min_count=1) # min_count = 1 means consider words even if it occurred only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUS88ry9k0VG",
    "outputId": "ceeb8a91-0b85-4e7b-e26c-5c21a14e1f5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# summarizing the loaded model\n",
    "print(mymodel)\n",
    "\n",
    "# Observation on below output\n",
    "# vocab - the total number of unique words\n",
    "# size - each of the below 14 vocab will be rated based on 100 features like for eg: does_bark, has_mouth, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vcyLQ1qak0VG"
   },
   "outputs": [],
   "source": [
    "# summarize vocabulary\n",
    "words = list(mymodel.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f85foqipk0VH",
    "outputId": "f55789db-8413-4cdc-c00d-d82700acae83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'This', 'is', 'python', 'training', 'by', 'Aman', 'Java', 'Data Science', 'Unfold', 'Data', 'Science', 'programming', '']\n"
     ]
    }
   ],
   "source": [
    "# summarize vocabulary\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RN0lLxNpk0VH",
    "outputId": "c9737e59-e6a5-4956-c1f2-fa9524c01e90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.6415866e-03 -1.3855502e-03 -3.8600569e-03 -4.9171047e-03\n",
      "  6.8308087e-04  2.3193758e-03 -1.0291578e-03  2.3272699e-03\n",
      " -1.9845173e-03  1.5841145e-03  4.4977646e-03 -3.7625260e-03\n",
      "  3.2597310e-03 -2.4843819e-03  2.6844600e-03 -5.3768087e-04\n",
      " -2.1219174e-03 -4.0911287e-03 -3.9883726e-03 -4.8130397e-03\n",
      " -3.8309779e-03  2.7398660e-03  4.6871815e-04 -1.8282381e-03\n",
      " -1.4734179e-03  2.4689538e-03  4.0270435e-03 -3.2605149e-03\n",
      "  2.9342817e-03  2.7687694e-03 -1.6917753e-04  3.6806199e-03\n",
      " -2.5652875e-03  2.0396789e-03  4.1413107e-03  8.6929744e-05\n",
      " -4.6985513e-03 -1.7085299e-03  6.8564975e-04  6.5581390e-04\n",
      "  1.9432650e-03  2.1717893e-03  4.8512691e-03  4.6147895e-03\n",
      " -1.2325962e-03 -6.6940986e-05 -2.4595321e-03  3.5173693e-03\n",
      "  2.5457388e-03 -2.5102813e-03  2.2696787e-03  1.8118306e-03\n",
      " -9.0079533e-04 -3.9707045e-03  2.4799458e-03  4.1773377e-04\n",
      "  4.4813706e-03 -1.6630677e-03 -1.2754368e-03  4.1415193e-03\n",
      "  1.1745522e-03  1.9707424e-03 -4.8856792e-04 -4.3606777e-03\n",
      "  3.7441468e-03 -4.8587695e-03 -7.9203839e-04  4.7859573e-03\n",
      "  2.5124918e-03 -1.4441441e-03 -2.5697620e-04 -4.3698037e-03\n",
      " -2.3467094e-03  9.4523927e-04  1.3690924e-03 -2.6957103e-04\n",
      " -2.9410655e-03  4.9222037e-03 -3.7990909e-03  6.8748050e-04\n",
      " -2.9800083e-03 -3.4201418e-03  3.0164216e-03  1.5875376e-03\n",
      " -2.2654621e-04  4.8178476e-03  2.6875334e-03 -4.0156865e-03\n",
      " -2.4786903e-04 -1.4136657e-03  1.2264599e-03 -2.9007273e-03\n",
      "  5.8045145e-04 -1.2693258e-03  3.4010396e-03  2.0706863e-03\n",
      " -3.2418533e-03 -3.7835602e-04 -4.8172730e-03  4.0693381e-03]\n"
     ]
    }
   ],
   "source": [
    "# access word vector for one word \"Hello\"\n",
    "print(mymodel['Hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0D30_GvDk0VK",
    "outputId": "679b0924-b7bd-4355-d819-e0ed514d6108"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('programming', 0.13299772143363953),\n",
       " ('training', 0.08566539734601974),\n",
       " ('', 0.04990440607070923),\n",
       " ('Unfold', 0.03425639122724533),\n",
       " ('Java', 0.02375667728483677),\n",
       " ('is', -0.004918448626995087),\n",
       " ('Hello', -0.02265256643295288),\n",
       " ('by', -0.028853053227066994),\n",
       " ('Science', -0.04116513580083847),\n",
       " ('python', -0.05294158309698105)]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try finding most similar words for word \"Data\"\n",
    "mymodel.most_similar(\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "YtuZlGPHWzJy",
    "outputId": "0da047c2-26dc-432f-fccc-b11e20342eae"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5e418371aa35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#try finding most similar words for word \"data\", to check case sensitivity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmymodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 )\n\u001b[0;32m-> 1422\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \"\"\"\n\u001b[0;32m-> 1397\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'data' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "#try finding most similar words for word \"data\", to check case sensitivity\n",
    "mymodel.most_similar(\"data\")\n",
    "\n",
    "# so if we try to find the most_similar matrix for a word not present in the training set (case insensitive), we will get error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AaQn_PK5XrNY"
   },
   "outputs": [],
   "source": [
    "### Drawback of 2. We can also train our own embedding using word2vec library.\n",
    "# as we have supplied limited words, training quality is not good enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl87RhNQk0VK"
   },
   "source": [
    "# First Task for you - comment me how you can solve a document classification problem using above concept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RurIQnNPk0VL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTKAE9qEk0VL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kV84FD6k0VL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAVwg8Q-k0VM"
   },
   "outputs": [],
   "source": [
    "#####################################Part 2###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T16iv-9ck0VM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvShjyZ6k0VN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRw9_BeKk0VR"
   },
   "source": [
    "# Option 3 - Create Embedding model using Keras Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mNg3CoKOk0VT"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding # This layer can only be used as the first layer in a model.\n",
    "# define documents\n",
    "Sent = ['Hello, how are you',\n",
    "        'how are you',\n",
    "        'how are you doing',\n",
    "        'I am doing great',\n",
    "        'I am doing good',\n",
    "        'We are good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YI7Al5gmk0VU"
   },
   "outputs": [],
   "source": [
    "# defining class labels: first 3 docs as class-1, next 3 docs as class-0\n",
    "sent_labels = array([1,1,1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLG2Jnfgk0VV",
    "outputId": "58196a97-8b09-48e2-9279-a8649d926cfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29, 22, 3, 13], [22, 3, 13], [22, 3, 13, 8], [3, 5, 8, 27], [3, 5, 8, 26], [23, 3, 26]]\n"
     ]
    }
   ],
   "source": [
    "# integer encoding of the documents\n",
    "my_vocab_size = 30 # here we want the vocab size (in last eg it was 100 by default) to be 30\n",
    "encoded_sent = [one_hot(i, my_vocab_size) for i in Sent] # one hot encoding of sentences\n",
    "print(encoded_sent)\n",
    "\n",
    "# Observation: 'are' has been encoded with 3, but 'i' is also getting represented by 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bwTog68k0VV",
    "outputId": "c7bcdaea-6bf3-4174-c1a9-df871d140987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 29 22  3 13]\n",
      " [ 0  0 22  3 13]\n",
      " [ 0 22  3 13  8]\n",
      " [ 0  3  5  8 27]\n",
      " [ 0  3  5  8 26]\n",
      " [ 0  0 23  3 26]]\n"
     ]
    }
   ],
   "source": [
    "# padding documents to a max length =5 \n",
    "# This is to make the 'encoded_sent' vector nXn matrix.\n",
    "length = 5\n",
    "padded_sent = pad_sequences(encoded_sent, maxlen=length, padding='pre') # max_length is 5, to get a 5X5 matrix. and 0s will be added to the beginning\n",
    "print(padded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "i4mnqMvAk0VW"
   },
   "outputs": [],
   "source": [
    "# defining the model\n",
    "mymodel = Sequential() # defining a sequential model\n",
    "mymodel.add(Embedding(my_vocab_size, 8, input_length=length)) # adding an embedding layer\n",
    "# my_vocab_size - size of my vocabulary\n",
    "# 8 - seems like output dimension\n",
    "# max_length - the padding size, n in nXn matrix\n",
    "mymodel.add(Flatten())\n",
    "mymodel.add(Dense(1, activation='sigmoid')) # output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "h4xXEGZmk0VX"
   },
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "mymodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSC96LQDk0VX"
   },
   "outputs": [],
   "source": [
    "# fiting  the model\n",
    "mymodel.fit(padded_sent, sent_labels, epochs=30)\n",
    "\n",
    "# evaluate the model\n",
    "modelloss, modelaccuracy = mymodel.evaluate(padded_sent, sent_labels, verbose=0)\n",
    "print('Accuracy: %f' % (modelaccuracy*100))\n",
    "\n",
    "# here the accuracy is 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgxyCfQhk0VX"
   },
   "source": [
    "# The Prediction part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "8QaaVpd5k0VY"
   },
   "outputs": [],
   "source": [
    "mysent_to_predict = ['how are you Suman',\n",
    "        'I am good']\n",
    "\n",
    "# question should be classified as 1, and answer should be classified as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUiul4lAk0VZ",
    "outputId": "aa8ae4f7-1f60-46a6-ef7a-17465f93970b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22, 3, 13, 29], [3, 5, 26]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = 30\n",
    "encoded = [one_hot(d, vocab_size) for d in mysent_to_predict]\n",
    "print(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qvvbxrwqk0Vc",
    "outputId": "b4406e85-1f48-478a-9cfe-f5f1b3b063dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 22  3 13 29]\n",
      " [ 0  0  3  5 26]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 5 words\n",
    "max_length = 5\n",
    "mypadded = pad_sequences(encoded, maxlen=max_length, padding='pre')\n",
    "print(mypadded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-JzT7APk0Vc",
    "outputId": "b6473659-e8ac-4662-cdbc-3f26fc058f5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.predict_classes(mypadded)\n",
    "\n",
    "# Here even 'suman' is a new word, the model didn't gave any error like the earlier one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWGrjhxRk0Vj"
   },
   "source": [
    "# Option 1 - Using Pre Trained Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "dziblZYtk0Vm",
    "outputId": "182204c8-7925-4448-86fd-a7d491fa007a"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-8fada69843f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F:\\\\\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'KeyedVector' from 'gensim.models' (/usr/local/lib/python3.7/dist-packages/gensim/models/__init__.py)",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVector   \n",
    "import os \n",
    "os.chdir(\"F:\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0NiHagKk0Vo"
   },
   "outputs": [],
   "source": [
    "#Download GoogleNews-vectors-negative300.bin from ; this is a pre-trained model\n",
    "#https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMijFTuek0Vo"
   },
   "outputs": [],
   "source": [
    "PreTrainedModel = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaCi2Yt1k0Vp",
    "outputId": "4f373401-db18-4120-924e-7bb3fe28b1e7"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c46f59eba3c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# calculate: (king - man) + woman = ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    341\u001b[0m             \u001b[0mnegative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36minit_sims\u001b[1;34m(self, replace)\u001b[0m\n\u001b[0;32m   1046\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# calculate: (king - man) + woman = ?\n",
    "result = PreTrainedModel.most_similar(\"Data\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTjvZAK0k0Vq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Creating Word Embedding Using Gensim.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
