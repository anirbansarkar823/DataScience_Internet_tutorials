{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Ada-Boost Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23IeMRuuwVRW"
      },
      "source": [
        "# Implementation of Ada-Boost Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8lgDoPFwVRe"
      },
      "source": [
        "#Importing neccesary packages\n",
        "# Load libraries\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU5hS26EwVRh"
      },
      "source": [
        "# Load data\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOUy85oSz9xh",
        "outputId": "ea143819-7297-4789-e873-89c65a2dbe49"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg0V_dPiwVRi"
      },
      "source": [
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # 80% training and 20% test"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRYB5pMxwVRi"
      },
      "source": [
        "# Create adaboost classifer object\n",
        "AdaModel = AdaBoostClassifier(n_estimators=100,\n",
        "                         learning_rate=1) #here stem/base_estimator is DecisionTreeClassifier\n",
        "# Train Adaboost Classifer\n",
        "model = AdaModel.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qr1LZyjwVRj"
      },
      "source": [
        "#Important Parameters\n",
        "base_estimator: It is a weak learner used to train the model. It uses DecisionTreeClassifier as default weak learner for training purpose. You can also specify different machine learning algorithms.\n",
        "\n",
        "n_estimators: Number of weak learners to train iteratively.\n",
        "\n",
        "learning_rate: It contributes to the weights of weak learners. It uses 1 as a default value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUO_A1olwVRk",
        "outputId": "acb973e6-4509-46d7-ae10-a7108679f31e"
      },
      "source": [
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK3qZ2TvwVRn"
      },
      "source": [
        "# Import Support Vector Classifier\n",
        "from sklearn.svm import SVC\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "svc=SVC(probability=True, kernel='poly') #{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’},\n",
        "\n",
        "# Create adaboost classifer object\n",
        "abc =AdaBoostClassifier(n_estimators=100, base_estimator=svc,learning_rate=1)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLv0BapbwVRo"
      },
      "source": [
        "# Train Adaboost Classifer\n",
        "model = abc.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ad9EAzNwVRp",
        "outputId": "b043a15d-ef85-4bc2-bdf4-6f51e22dffd6"
      },
      "source": [
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYmpUpohzuXH"
      },
      "source": [
        "# solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
        "# Algorithm to use in the optimization problem.\n",
        "\n",
        "# For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n",
        "\n",
        "# For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
        "\n",
        "# ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n",
        "\n",
        "# ‘liblinear’ and ‘saga’ also handle L1 penalty\n",
        "\n",
        "# ‘saga’ also supports ‘elasticnet’ penalty\n",
        "\n",
        "# ‘liblinear’ does not support setting penalty='none'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztakKLPPylx4"
      },
      "source": [
        "# use of logistic regression as base_classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model = LogisticRegression(solver='newton-cg')\n",
        "\n",
        "ada_lr = AdaBoostClassifier(n_estimators = 100, base_estimator= lr_model, learning_rate=1)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKZ1BFVxyo-n"
      },
      "source": [
        "# Train Adaboost Classifer\n",
        "model = ada_lr.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5sMmViQyq__",
        "outputId": "184f40c1-a232-40f9-9731-100c953410fc"
      },
      "source": [
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOjKJI_hwVRq"
      },
      "source": [
        "Pros\n",
        "AdaBoost is easy to implement. It iteratively corrects the mistakes of the weak classifier and improves accuracy by combining weak learners. You can use many base classifiers with AdaBoost. AdaBoost is not prone to overfitting. This can be found out via experiment results, but there is no concrete reason available.\n",
        "\n",
        "Cons\n",
        "AdaBoost is sensitive to noise data. It is highly affected by outliers because it tries to fit each point perfectly. AdaBoost is slower compared to XGBoost."
      ]
    }
  ]
}