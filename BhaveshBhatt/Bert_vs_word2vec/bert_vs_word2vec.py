# -*- coding: utf-8 -*-
"""BERT_vs_Word2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OkqFWi6sxn-mtdkLgWQFSMrKW6UzP35G

### [Youtube tutorial by Bhavesh Bhatt](https://www.youtube.com/watch?v=1W-sWmFQPZY)
# BERT tutorial
"""

# installing one nlp library
!pip install flair

# importing required libraries
import numpy as np
from flair.embeddings import WordEmbeddings
from flair.embeddings import TransformerWordEmbeddings
from flair.data import Sentence
from scipy.spatial import distance

# one word vector for each word; this are context independent
# This means, a word, even if it has occurred in different sentences with different meanings will get the same numerical representation
# we use here glove embeddings
glove_embedding = WordEmbeddings('glove')

# this word to vec model is context independent
sent_1 = Sentence("oneplus released oneplus x pro in 2021")

# now we will convert the word into vector representation using the glove model
glove_embedding.embed(sent_1)

# now let us see each word in the form of vector
# representing each word in the form of vector
for token in sent_1:
    print(token)
    print(token.embedding)
    print("\n")

# shape of each word vector
sent_1[0].embedding.shape

# Let us create another sentence with similar words
sent_2 = Sentence("oneplus Oneplus one is three")

# converting word to vector
glove_embedding.embed(sent_2)

distance.euclidean(np.array(sent_2[0].embedding), np.array(sent_2[1].embedding)) # there is no difference between 'oneplus' and 'Oneplus'

# let us visualize the vector representation of words
for token in sent_2:
    print(token)
    print(token.embedding)
    print("\n")

# shape of each word
sent_2[0].embedding.shape

# now let us see if there any different among the tensors of word 'oneplue' with different contexts

glove_dst = distance.euclidean(np.array(sent_1[0].embedding), np.array(sent_2[0].embedding))
print(glove_dst)

# So in word2vec, word embedding doesn't change with context

"""# Bert Embeddings"""

# downloading the bert model
bert_embedding = TransformerWordEmbeddings('bert-base-multilingual-cased')

# embedding sentence 1
bert_embedding.embed(sent_1)
for token in sent_1:
    print(token)
    print(token.embedding)

# let us see the tensors for each word embeddings
sent_1[0].embedding.shape

# tensors are larger in size than word embedding

# embedding the second sentence
bert_embedding.embed(sent_2)

for token in sent_2:
    print(token)
    print(token.embedding)

sent_2[0].embedding.shape

# let us see if there is any difference between the the two 'oneplus' used in two different contexts
bert_dst = distance.euclidean(np.array(sent_1[0].embedding), np.array(sent_2[0].embedding))
print(bert_dst)

# This shows bert embeddings captures contexts